{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee5d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1552e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25398d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lic.constants import (\n",
    "    PROJECT_ROOT,\n",
    "    LOGS_DIR,\n",
    "    GPT_4O_MINI,\n",
    "    GPT_5_MINI,\n",
    ")\n",
    "from collabmem.constants import REPO_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3acb66f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_DIR = REPO_ROOT / \"src/lic/logs\"\n",
    "# !ls {LOGS_DIR / \"actions/full\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39552023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path: Path) -> list:\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4bcdb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_file_parsing\n",
    "# full_actions_gpt-5-mini_2025-08-07.jsonl\n",
    "# {setting}_{task}_{model}.jsonl\n",
    "# log_file_pattern = re.compile(r'^(?P<setting>.+)_(?P<task>.+)_(?P<model>.+)\\.jsonl$')\n",
    "# need to rework the regex: only \"model\" can have underscores\n",
    "log_file_pattern = re.compile(\n",
    "    r'^(?P<setting>[^_]+)_(?P<task>[^_]+)_(?P<model>.+)\\.jsonl$'\n",
    ")\n",
    "\n",
    "def parse_log_directory(log_dir: Path, testing: bool = False) -> defaultdict:\n",
    "    # task, setting, model -> outputs\n",
    "    # all_outputs = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    all_outputs: dict[str, dict[str, dict[str, list]]] = {}\n",
    "    for task_dir in log_dir.iterdir():\n",
    "        if not task_dir.is_dir():\n",
    "            continue\n",
    "        task = task_dir.name\n",
    "        if task not in all_outputs:\n",
    "            all_outputs[task] = {}\n",
    "        for setting_dir in task_dir.iterdir():\n",
    "            if not setting_dir.is_dir():\n",
    "                continue\n",
    "            setting = setting_dir.name\n",
    "            if setting not in all_outputs[task]:\n",
    "                all_outputs[task][setting] = {}\n",
    "            for file_path in setting_dir.iterdir():\n",
    "                if file_path.suffix != '.jsonl':\n",
    "                    continue\n",
    "                # file path format:\n",
    "                # full_actions_gpt-5-mini_2025-08-07.jsonl\n",
    "                # {setting}_{task}_{model}.jsonl\n",
    "                \n",
    "\n",
    "            \n",
    "                match = log_file_pattern.match(file_path.name)\n",
    "                if not match:\n",
    "                    print(f\"Skipping unrecognized file: {file_path}\")\n",
    "                    continue\n",
    "                if testing:\n",
    "                    print(file_path.name)\n",
    "                    print(f\"groups: {match.groupdict()}\")\n",
    "                    return\n",
    "                model = match.group('model')\n",
    "                outputs = load_jsonl(file_path)\n",
    "                all_outputs[task][setting][model] = outputs\n",
    "    return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c46bd9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_logs = parse_log_directory(LOGS_DIR, testing=True)\n",
    "parsed_logs = parse_log_directory(LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb20d2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['math', 'code'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6db5cfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['full', 'sharded'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs[\"math\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c54820e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['local_cl_math', 'local_base_llama'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs[\"math\"][\"full\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "699b3a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = \"local_base_llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03972daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'list'>,\n",
      "length: 24,\n",
      "first entry:\n",
      "dict_keys(['conv_id', 'conv_type', 'task', 'task_id', 'dataset_fn', 'assistant_model', 'system_model', 'user_model', 'git_version', 'trace', 'is_correct', 'score'])\n"
     ]
    }
   ],
   "source": [
    "dummy = parsed_logs[\"math\"][\"full\"][dummy_model]\n",
    "print(f\"type: {type(dummy)},\\nlength: {len(dummy)},\\nfirst entry:\\n{dummy[0].keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc02b66e",
   "metadata": {},
   "source": [
    "## Scoring Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "414504e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for local_base_llama on math/full: 0.71 over 24 entries\n"
     ]
    }
   ],
   "source": [
    "# score dummy first\n",
    "total_entries = len(dummy)\n",
    "total_score = 0\n",
    "for entry in dummy:\n",
    "    total_score += entry.get(\"score\", 0)\n",
    "average_score = total_score / total_entries\n",
    "print(f\"Average score for {dummy_model} on math/full: {average_score:.2f} over {total_entries} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "325b484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring function for list[traces]\n",
    "def score_task_entries(\n",
    "    entries: list[dict],\n",
    "    score_key: str = \"score\"\n",
    ") -> tuple[float, int, int]:\n",
    "    total_entries = len(entries)\n",
    "    if total_entries == 0:\n",
    "        return 0.0\n",
    "    total_score = 0\n",
    "    none_count = 0\n",
    "    for entry in entries:\n",
    "        score = entry.get(score_key, 0)\n",
    "        if score is None:\n",
    "            none_count += 1\n",
    "        else:\n",
    "            total_score += score\n",
    "    average_score = total_score / total_entries\n",
    "    return average_score, none_count, total_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96abbc7",
   "metadata": {},
   "source": [
    "#### Initial GPT-5-mini Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01c09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Task</th>\n",
       "      <th>summary</th>\n",
       "      <th>math</th>\n",
       "      <th>data2text</th>\n",
       "      <th>code</th>\n",
       "      <th>actions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Setting</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sharded</th>\n",
       "      <td>0.080628</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.274894</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.523810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full</th>\n",
       "      <td>0.109325</td>\n",
       "      <td>0.883495</td>\n",
       "      <td>0.376260</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Task      summary      math  data2text      code   actions\n",
       "Setting                                                   \n",
       "sharded  0.080628  0.627451   0.274894  0.367347  0.523810\n",
       "full     0.109325  0.883495   0.376260  0.490000  0.885714"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a pandas DataFrame of scores\n",
    "# columns: task\n",
    "# rows: setting\n",
    "# for now, fix model to GPT_5_MINI\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for task, settings in parsed_logs.items():\n",
    "    for setting, models in settings.items():\n",
    "        entries = models.get(GPT_5_MINI, [])\n",
    "        avg_score, none_counts, total_entries = score_task_entries(entries)\n",
    "        df.loc[setting, task] = avg_score\n",
    "df.index.name = \"Setting\"\n",
    "df.columns.name = \"Task\"\n",
    "df = df.astype(float)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ac93f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Setting   |   collabllm_math |   base_llama |\n",
      "|:----------|-----------------:|-------------:|\n",
      "| full      |         0.708333 |     0.708333 |\n",
      "| sharded   |         0.416667 |     0.458333 |\n"
     ]
    }
   ],
   "source": [
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03ccfc",
   "metadata": {},
   "source": [
    "#### Pretrained CollabLLM Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51d3946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for task: code\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>collabllm_code</th>\n",
       "      <th>base_llama</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Setting</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>full</th>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sharded</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model    collabllm_code  base_llama\n",
       "Setting                            \n",
       "full           0.541667    0.521739\n",
       "sharded        0.250000    0.416667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Setting   |   collabllm_code |   base_llama |\n",
      "|:----------|-----------------:|-------------:|\n",
      "| full      |         0.541667 |     0.521739 |\n",
      "| sharded   |         0.25     |     0.416667 |\n",
      "Results for task: math\n",
      "Warning: 2 entries with None score for local_cl_math on math/sharded\n",
      "Warning: 1 entries with None score for local_base_llama on math/sharded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>collabllm_math</th>\n",
       "      <th>base_llama</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Setting</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>full</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sharded</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model    collabllm_math  base_llama\n",
       "Setting                            \n",
       "full           0.708333    0.708333\n",
       "sharded        0.416667    0.458333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Setting   |   collabllm_math |   base_llama |\n",
      "|:----------|-----------------:|-------------:|\n",
      "| full      |         0.708333 |     0.708333 |\n",
      "| sharded   |         0.416667 |     0.458333 |\n"
     ]
    }
   ],
   "source": [
    "# results I need to visualize this time:\n",
    "# (code, math) x (full, sharded) x (base_llama, cl_*) where * corresponds to the (code, math) selection\n",
    "# let's do 2 tables, one for code and one for math\n",
    "# let's keep setting as rows and then replace columns task->model\n",
    "# again let's make dataframes for easier visualization\n",
    "\n",
    "for task in [\"code\", \"math\"]:\n",
    "    print(f\"Results for task: {task}\")\n",
    "    df = pd.DataFrame()\n",
    "    for setting, models in parsed_logs[task].items():\n",
    "        for model_name, entries in models.items():\n",
    "            avg_score, none_counts, total_entries = score_task_entries(entries)\n",
    "            # print none counts\n",
    "            if none_counts > 0:\n",
    "                print(f\"Warning: {none_counts} entries with None score for {model_name} on {task}/{setting}\")\n",
    "            model_name = model_name.replace(\"local_\", \"\").replace(\"cl_\", \"collabllm_\")\n",
    "            df.loc[setting, model_name] = avg_score\n",
    "    df.index.name = \"Setting\"\n",
    "    df.columns.name = \"Model\"\n",
    "    df = df.astype(float)\n",
    "    display(df)\n",
    "    # also print df to markdown to copy paste into docsA\n",
    "    print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba6435e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_math=\"/home/v-homatthew/collabmem/outputs/offline_dpo_from_base/collabllm-multiturn-gsm8k-r3\" dpo_code=\"/home/v-homatthew/collabmem/outputs/offline_dpo_from_base/collabllm-multiturn-lic-code\" dpo_actions=\"/home/v-homatthew/collabmem/outputs/offline_dpo_from_base/collabllm-multiturn-bfcl\" dpo_database=\"/home/v-homatthew/collabmem/outputs/offline_dpo_from_base/collabllm-multiturn-spider\" dpo_data2text=\"/home/v-homatthew/collabmem/outputs/offline_dpo_from_base/collabllm-multiturn-totto\"\n"
     ]
    }
   ],
   "source": [
    "def prep_serve_olmo_adapters():\n",
    "    adapter_directory = Path(\"/home/v-homatthew/collabmem/outputs/offline_dpo_from_base\")\n",
    "    # !ls {Path(\"/home/v-homatthew/collabmem/outputs/offline_dpo_from_base\")}\n",
    "    # collabllm-multiturn-bfcl      collabllm-multiturn-lic-code\n",
    "    # collabllm-multiturn-gsm8k     collabllm-multiturn-spider\n",
    "    # collabllm-multiturn-gsm8k-r3  collabllm-multiturn-totto\n",
    "\n",
    "    # mapping {math: gsm8k-r3, code: lic-code, actions: bfcl, database: spider, data2text: totto}\n",
    "    # create actual python mapping of these keys to the corresponding path\n",
    "    task2adapter_path = {\n",
    "        \"math\": adapter_directory / \"collabllm-multiturn-gsm8k-r3\",\n",
    "        \"code\": adapter_directory / \"collabllm-multiturn-lic-code\",\n",
    "        \"actions\": adapter_directory / \"collabllm-multiturn-bfcl\",\n",
    "        \"database\": adapter_directory / \"collabllm-multiturn-spider\",\n",
    "        \"data2text\": adapter_directory / \"collabllm-multiturn-totto\",\n",
    "    }\n",
    "\n",
    "    # need a string containing\n",
    "    entries = [f'dpo_{k}=\"{v}\"' for k, v in task2adapter_path.items()]\n",
    "    print(\" \".join(entries))\n",
    "\n",
    "prep_serve_olmo_adapters()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colmem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

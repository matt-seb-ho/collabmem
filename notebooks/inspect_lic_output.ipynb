{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee5d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1552e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25398d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import (\n",
    "    PROJECT_ROOT,\n",
    "    LOGS_DIR,\n",
    "    GPT_4O_MINI,\n",
    "    GPT_5_MINI,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3acb66f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_actions_gpt-5-mini_2025-08-07.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls {LOGS_DIR / \"actions/full\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39552023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path: Path) -> list:\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4bcdb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_file_parsing\n",
    "# full_actions_gpt-5-mini_2025-08-07.jsonl\n",
    "# {setting}_{task}_{model}.jsonl\n",
    "# log_file_pattern = re.compile(r'^(?P<setting>.+)_(?P<task>.+)_(?P<model>.+)\\.jsonl$')\n",
    "# need to rework the regex: only \"model\" can have underscores\n",
    "log_file_pattern = re.compile(\n",
    "    r'^(?P<setting>[^_]+)_(?P<task>[^_]+)_(?P<model>.+)\\.jsonl$'\n",
    ")\n",
    "\n",
    "def parse_log_directory(log_dir: Path, testing: bool = False) -> defaultdict:\n",
    "    # task, setting, model -> outputs\n",
    "    # all_outputs = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    all_outputs: dict[str, dict[str, dict[str, list]]] = {}\n",
    "    for task_dir in log_dir.iterdir():\n",
    "        if not task_dir.is_dir():\n",
    "            continue\n",
    "        task = task_dir.name\n",
    "        if task not in all_outputs:\n",
    "            all_outputs[task] = {}\n",
    "        for setting_dir in task_dir.iterdir():\n",
    "            if not setting_dir.is_dir():\n",
    "                continue\n",
    "            setting = setting_dir.name\n",
    "            if setting not in all_outputs[task]:\n",
    "                all_outputs[task][setting] = {}\n",
    "            for file_path in setting_dir.iterdir():\n",
    "                if file_path.suffix != '.jsonl':\n",
    "                    continue\n",
    "                # file path format:\n",
    "                # full_actions_gpt-5-mini_2025-08-07.jsonl\n",
    "                # {setting}_{task}_{model}.jsonl\n",
    "                \n",
    "\n",
    "            \n",
    "                match = log_file_pattern.match(file_path.name)\n",
    "                if not match:\n",
    "                    print(f\"Skipping unrecognized file: {file_path}\")\n",
    "                    continue\n",
    "                if testing:\n",
    "                    print(file_path.name)\n",
    "                    print(f\"groups: {match.groupdict()}\")\n",
    "                    return\n",
    "                model = match.group('model')\n",
    "                outputs = load_jsonl(file_path)\n",
    "                all_outputs[task][setting][model] = outputs\n",
    "    return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46bd9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_logs = parse_log_directory(LOGS_DIR, testing=True)\n",
    "parsed_logs = parse_log_directory(LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb20d2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['summary', 'math', 'data2text', 'code', 'actions'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6db5cfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sharded', 'full'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs[\"math\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c54820e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gpt-5-mini_2025-08-07'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_logs[\"math\"][\"full\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03972daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'list'>,\n",
      "length: 103,\n",
      "first entry:\n",
      "dict_keys(['conv_id', 'conv_type', 'task', 'task_id', 'dataset_fn', 'assistant_model', 'system_model', 'user_model', 'git_version', 'trace', 'is_correct', 'score'])\n"
     ]
    }
   ],
   "source": [
    "dummy = parsed_logs[\"math\"][\"full\"][GPT_5_MINI]\n",
    "print(f\"type: {type(dummy)},\\nlength: {len(dummy)},\\nfirst entry:\\n{dummy[0].keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc02b66e",
   "metadata": {},
   "source": [
    "## Scoring Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "414504e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for gpt-5-mini_2025-08-07 on math/full: 0.88 over 103 entries\n"
     ]
    }
   ],
   "source": [
    "# score dummy first\n",
    "total_entries = len(dummy)\n",
    "total_score = 0\n",
    "for entry in dummy:\n",
    "    total_score += entry.get(\"score\", 0)\n",
    "average_score = total_score / total_entries\n",
    "print(f\"Average score for {GPT_5_MINI} on math/full: {average_score:.2f} over {total_entries} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "325b484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring function for list[traces]\n",
    "def score_task_entries(\n",
    "    entries: list[dict],\n",
    "    score_key: str = \"score\"\n",
    ") -> tuple[float, int, int]:\n",
    "    total_entries = len(entries)\n",
    "    if total_entries == 0:\n",
    "        return 0.0\n",
    "    total_score = 0\n",
    "    none_count = 0\n",
    "    for entry in entries:\n",
    "        score = entry.get(score_key, 0)\n",
    "        if score is None:\n",
    "            none_count += 1\n",
    "        else:\n",
    "            total_score += score\n",
    "    average_score = total_score / total_entries\n",
    "    return average_score, none_count, total_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01c09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Task</th>\n",
       "      <th>summary</th>\n",
       "      <th>math</th>\n",
       "      <th>data2text</th>\n",
       "      <th>code</th>\n",
       "      <th>actions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Setting</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sharded</th>\n",
       "      <td>0.080628</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.274894</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.523810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full</th>\n",
       "      <td>0.109325</td>\n",
       "      <td>0.883495</td>\n",
       "      <td>0.376260</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Task      summary      math  data2text      code   actions\n",
       "Setting                                                   \n",
       "sharded  0.080628  0.627451   0.274894  0.367347  0.523810\n",
       "full     0.109325  0.883495   0.376260  0.490000  0.885714"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a pandas DataFrame of scores\n",
    "# columns: task\n",
    "# rows: setting\n",
    "# for now, fix model to GPT_5_MINI\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for task, settings in parsed_logs.items():\n",
    "    for setting, models in settings.items():\n",
    "        entries = models.get(GPT_5_MINI, [])\n",
    "        avg_score, none_counts, total_entries = score_task_entries(entries)\n",
    "        df.loc[setting, task] = avg_score\n",
    "df.index.name = \"Setting\"\n",
    "df.columns.name = \"Task\"\n",
    "df = df.astype(float)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d85deb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Setting   |   summary |     math |   data2text |     code |   actions |\n",
      "|:----------|----------:|---------:|------------:|---------:|----------:|\n",
      "| sharded   | 0.0806277 | 0.627451 |    0.274894 | 0.367347 |  0.52381  |\n",
      "| full      | 0.109325  | 0.883495 |    0.37626  | 0.49     |  0.885714 |\n"
     ]
    }
   ],
   "source": [
    "print(df.to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colmem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# lic/cheatsheet_context_editor.py
from lic.model import generate
from lic.utils import extract_conversation

DEFAULT_EDITOR_SYSTEM_PROMPT = """You are a conversation state editor for multi-turn problem solving.

Your job: rewrite the conversation state so the next assistant turn is NOT biased by earlier incorrect assumptions or premature answers.

Rules:
- Do NOT add new facts.
- Treat prior ASSISTANT claims as untrusted unless supported by USER or SYSTEM messages.
- Prefer concise, structured state.
- Keep all user-provided constraints, data, and definitions.
- Explicitly call out: (a) confirmed facts, (b) uncertain/assumed items, (c) contradictions, (d) what is still missing.
- If prior assistant answers were wrong or premature, mark them as "discarded".

Output MUST be valid JSON with keys:
{
  "confirmed_facts": [string, ...],
  "constraints": [string, ...],
  "open_questions": [string, ...],
  "discarded_assumptions": [string, ...],
  "clean_context": string
}

"clean_context" should be a short narrative + bullet list state that can be shown to the next assistant.
"""


def edit_conversation_state(
    trace,
    editor_model="gpt-4o-mini",
    temperature=0.0,
    max_tokens=1200,
    editor_preamble: str | None = None,  # NEW
):
    """
    Returns:
      edited_state (dict): parsed JSON from the editor model
      editor_obj (dict): raw generate() return_metadata object
    """
    messages = extract_conversation(trace, to_str=False)

    editor_messages = [{"role": "system", "content": DEFAULT_EDITOR_SYSTEM_PROMPT}]

    # NEW: inject editor cheatsheet/preamble as extra system guidance
    if editor_preamble:
        editor_messages.append(
            {
                "role": "system",
                "content": (
                    "EDITOR CHEATSHEET (persistent guidance for editing conversation state):\n"
                    f"{editor_preamble}"
                ),
            }
        )

    editor_messages.append(
        {
            "role": "user",
            "content": (
                "Here is the conversation so far as a JSON-like message list.\n\n"
                f"{messages}\n\n"
                "Return the edited state JSON now."
            ),
        }
    )

    editor_obj = generate(
        editor_messages,
        model=editor_model,
        temperature=temperature,
        return_metadata=True,
        max_tokens=max_tokens,
    )

    text = editor_obj["message"]

    import json

    try:
        edited_state = json.loads(text)
    except Exception:
        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1 and end > start:
            edited_state = json.loads(text[start : end + 1])
        else:
            raise ValueError(f"Context editor did not return JSON. Raw:\n{text}")

    return edited_state, editor_obj


def build_assistant_input_from_edited_state(
    original_system_message: str,
    last_user_message: str,
    edited_state: dict,
    assistant_preamble: str | None = None,  # NEW
):
    """
    Build the message list passed to the assistant model.

    We keep the task system message first.
    Then optionally inject assistant cheatsheet/preamble as another system message.
    Then add the edited state as another system message.
    Finally include the last user message verbatim.
    """
    clean_context = edited_state.get("clean_context", "")

    edited_system = (
        "Edited conversation state (generated by a context editor). "
        "Treat it as a summary of USER-confirmed information and explicit uncertainties.\n\n"
        f"{clean_context}"
    )

    out = [{"role": "system", "content": original_system_message}]

    if assistant_preamble:
        out.append({"role": "system", "content": assistant_preamble})

    out.extend(
        [
            {"role": "system", "content": edited_system},
            {"role": "user", "content": last_user_message},
        ]
    )
    return out

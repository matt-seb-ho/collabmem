# lic/context_editor.py
from lic.model import generate
from lic.utils import extract_conversation

DEFAULT_EDITOR_SYSTEM_PROMPT = """You are a conversation state editor for multi-turn problem solving.

Your job: rewrite the conversation state so the next assistant turn is NOT biased by earlier incorrect assumptions or premature answers.

Rules:
- Do NOT add new facts.
- Treat prior ASSISTANT claims as untrusted unless supported by USER or SYSTEM messages.
- Prefer concise, structured state.
- Keep all user-provided constraints, data, and definitions.
- Explicitly call out: (a) confirmed facts, (b) uncertain/assumed items, (c) contradictions, (d) what is still missing.
- If prior assistant answers were wrong or premature, mark them as "discarded".

Output MUST be valid JSON with keys:
{
  "confirmed_facts": [string, ...],
  "constraints": [string, ...],
  "open_questions": [string, ...],
  "discarded_assumptions": [string, ...],
  "clean_context": string
}

"clean_context" should be a short narrative + bullet list state that can be shown to the next assistant.
"""


def edit_conversation_state(
    trace,
    editor_model="gpt-4o-mini",
    temperature=0.0,
    max_tokens=1200,
):
    """
    Returns:
      edited_state (dict): parsed JSON from the editor model
      editor_obj (dict): raw generate() return_metadata object
    """
    # Convert trace -> messages
    messages = extract_conversation(trace, to_str=False)

    editor_messages = [
        {"role": "system", "content": DEFAULT_EDITOR_SYSTEM_PROMPT},
        {
            "role": "user",
            "content": (
                "Here is the conversation so far as a JSON-like message list.\n\n"
                f"{messages}\n\n"
                "Return the edited state JSON now."
            ),
        },
    ]

    editor_obj = generate(
        editor_messages,
        model=editor_model,
        temperature=temperature,
        return_metadata=True,
        max_tokens=max_tokens,
    )

    text = editor_obj["message"]

    # Robust-ish JSON extraction: keep it simple for baseline.
    # If your generate() already supports response_format/json, use that instead.
    import json

    try:
        edited_state = json.loads(text)
    except Exception:
        # Fallback: try to find the first {...} block
        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1 and end > start:
            edited_state = json.loads(text[start : end + 1])
        else:
            raise ValueError(f"Context editor did not return JSON. Raw:\n{text}")

    return edited_state, editor_obj


def build_assistant_input_from_edited_state(
    original_system_message: str,
    last_user_message: str,
    edited_state: dict,
):
    """
    Build the message list passed to the assistant model.

    We keep the task system message as the first system message.
    Then we add a second system message that provides the cleaned state.
    Finally we include the last user message verbatim.
    """
    clean_context = edited_state.get("clean_context", "")

    edited_system = (
        "Edited conversation state (generated by a context editor). "
        "Treat it as a summary of USER-confirmed information and explicit uncertainties.\n\n"
        f"{clean_context}"
    )

    return [
        {"role": "system", "content": original_system_message},
        {"role": "system", "content": edited_system},
        {"role": "user", "content": last_user_message},
    ]
